{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9441524,"sourceType":"datasetVersion","datasetId":5737592}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets\n!pip install langchain_openai\n!pip install nltk\n!pip install rank_bm25\n!pip install langchain_text_splitters\n!pip install torch\n!pip install transformers\n!pip install sentence_transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport os\nimport gc\nfrom datasets import load_dataset\nfrom langchain_openai import AzureChatOpenAI\nfrom langchain_openai import AzureOpenAIEmbeddings\nimport pandas as pd\nfrom rank_bm25 import BM25Okapi\nimport nltk\nfrom rank_bm25 import BM25Okapi\nimport numpy as np\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nfrom nltk.tokenize import word_tokenize\nfrom torch.nn import DataParallel\nfrom sentence_transformers import SentenceTransformer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download(\"punkt\")\n\n# Azure Chat OpenAI Model (GPT-4o)\nazure_configs = {\n    \"base_url\": \"https://open-ai-aus.openai.azure.com/\",\n    \"model_deployment\": \"gpt-4o\",\n    \"model_name\": \"gpt-4o\",\n    \"embedding_model_name\": \"text-embedding-ada-002\",\n    \"openai_api_key\": \"\",\n    \"openai_api_version\": \"2023-03-15-preview\",\n}\n\nazure_model = AzureChatOpenAI(\n    openai_api_version=azure_configs[\"openai_api_version\"],\n    azure_deployment=azure_configs[\"model_deployment\"],\n    azure_endpoint=azure_configs[\"base_url\"],\n    api_key=azure_configs[\"openai_api_key\"],\n    temperature=0.0,\n)\n\nazure_embedding_model = AzureOpenAIEmbeddings(\n    model=azure_configs[\"embedding_model_name\"],\n    azure_endpoint=azure_configs[\"base_url\"],\n    api_key=azure_configs[\"openai_api_key\"],\n    openai_api_version=azure_configs[\"openai_api_version\"],\n)\n\nquery_tokenizer = AutoTokenizer.from_pretrained(\"facebook/dragon-plus-query-encoder\")\nquery_encoder = AutoModel.from_pretrained(\"facebook/dragon-plus-query-encoder\").to(\"cuda\")\ncontext_encoder = AutoModel.from_pretrained(\"facebook/dragon-plus-context-encoder\").to(\"cuda\")\n\ncontriever_model = SentenceTransformer(\n    \"facebook/contriever\",\n    trust_remote_code=True,\n).cuda()\n\ncontriever_ms_model = SentenceTransformer(\n    \"facebook/contriever-msmarco\",\n    trust_remote_code=True,\n).cuda()\n\nstella_model = SentenceTransformer(\n    \"dunzhang/stella_en_400M_v5\",\n    trust_remote_code=True,\n    device=\"cuda\",\n    config_kwargs={\"use_memory_efficient_attention\": False, \"unpad_inputs\": False}\n).cuda()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def retrieve_context(context, input_question, retriever=\"BM25\", top_k=5, batch_size=32):\n    def word_count(text):\n        return len(word_tokenize(text))\n\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=200,  # Chunk size is 200 words\n        chunk_overlap=20,\n        length_function=word_count,\n    )\n    \n    context_chunks = splitter.split_text(context)\n\n\n    if retriever == \"BM25\":\n        # BM25 retrieval\n        tokenized_chunks = [chunk.split() for chunk in context_chunks]\n        bm25 = BM25Okapi(tokenized_chunks)\n        query_tokens = input_question.split()\n        scores = bm25.get_scores(query_tokens)\n\n        # Get indices of the top k scores\n        top_k_indices = np.argsort(scores)[::-1][:top_k]\n        top_k_chunks = [context_chunks[i] for i in top_k_indices]\n\n        return top_k_chunks\n\n    elif retriever == \"Dragon\":\n        with torch.no_grad():\n            # Tokenize and encode the input query and move to GPU\n            query_input = query_tokenizer(input_question, return_tensors=\"pt\").to(\"cuda\")\n            query_emb = query_encoder(**query_input).last_hidden_state[:, 0, :]\n\n            scores_list = []\n            indices_list = []\n\n            for i in range(0, len(context_chunks), batch_size):\n                # Get the batch of context chunks and move to GPU\n                batch_chunks = context_chunks[i:i + batch_size]\n                ctx_input = query_tokenizer(\n                    batch_chunks, padding=True, truncation=True, max_length=512, return_tensors=\"pt\"\n                ).to(\"cuda\")\n\n                ctx_emb = context_encoder(**ctx_input).last_hidden_state[:, 0, :]\n\n                # Compute similarity scores using dot product on GPU\n                scores = torch.matmul(query_emb, ctx_emb.T)\n                scores = scores.squeeze(0)  # Ensure scores is 1D\n                scores_list.append(scores.cpu())  # Move scores to CPU before appending\n                indices_list.extend(range(i, min(i + batch_size, len(context_chunks))))\n\n                # Delete batch variables and clear GPU cache\n                del ctx_input, ctx_emb, scores\n                gc.collect()\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n\n            # Concatenate scores and get top k indices\n            all_scores = torch.cat(scores_list).cpu()  # Ensure scores are on CPU\n            top_k = min(top_k, len(all_scores))\n            top_k_indices = torch.topk(all_scores, k=top_k).indices\n            top_k_chunks = [context_chunks[indices_list[i]] for i in top_k_indices]\n\n            # Delete variables and clear GPU cache\n            del query_input, query_emb, scores_list, all_scores\n            gc.collect()\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n\n            return top_k_chunks\n        \n    elif retriever == \"Contriever\":\n        # Encode the input question\n        query_embedding = contriever_model.encode(\n            [input_question], show_progress_bar=False, convert_to_tensor=True, device=\"cuda\"\n        )\n\n        # Initialize lists to store embeddings and indices\n        embeddings_list = []\n        indices_list = []\n\n        # Encode context chunks in batches\n        for i in range(0, len(context_chunks), batch_size):\n            batch_chunks = context_chunks[i:i + batch_size]\n            # Encode the batch of context chunks\n            ctx_embeddings = contriever_model.encode(\n                batch_chunks, show_progress_bar=False, convert_to_tensor=True, device=\"cuda\"\n            )\n            embeddings_list.append(ctx_embeddings)\n            indices_list.extend(range(i, min(i + batch_size, len(context_chunks))))\n\n            # Clear GPU cache\n            del ctx_embeddings\n            gc.collect()\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n\n        # Concatenate all context embeddings\n        context_embeddings = torch.cat(embeddings_list, dim=0)\n\n        # Compute similarity scores\n        similarities = contriever_model.similarity(query_embedding, context_embeddings)\n        similarities = similarities.squeeze(0)  # Convert to 1D tensor\n        similarities = similarities.cpu().numpy()\n        indices_list = np.array(indices_list)\n\n        # Get top k indices and sort them\n        top_k = min(top_k, len(similarities))\n        top_k_indices = np.argsort(-similarities)[:top_k]\n        sorted_results = [(similarities[i], indices_list[i]) for i in top_k_indices]\n        top_k_chunks = [context_chunks[idx] for _, idx in sorted_results]\n\n        # Clear variables and GPU cache\n        del query_embedding, embeddings_list, context_embeddings, similarities\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n        return top_k_chunks\n\n    elif retriever == \"ContrieverMsmarco\":\n        # Encode the input question\n        query_embedding = contriever_ms_model.encode(\n            [input_question], show_progress_bar=False, convert_to_tensor=True, device=\"cuda\"\n        )\n\n        # Initialize lists to store embeddings and indices\n        embeddings_list = []\n        indices_list = []\n\n        # Encode context chunks in batches\n        for i in range(0, len(context_chunks), batch_size):\n            batch_chunks = context_chunks[i:i + batch_size]\n            # Encode the batch of context chunks\n            ctx_embeddings = contriever_ms_model.encode(\n                batch_chunks, show_progress_bar=False, convert_to_tensor=True, device=\"cuda\"\n            )\n            embeddings_list.append(ctx_embeddings)\n            indices_list.extend(range(i, min(i + batch_size, len(context_chunks))))\n\n            # Clear GPU cache\n            del ctx_embeddings\n            gc.collect()\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n\n        # Concatenate all context embeddings\n        context_embeddings = torch.cat(embeddings_list, dim=0)\n\n        # Compute similarity scores\n        similarities = contriever_ms_model.similarity(query_embedding, context_embeddings)\n        similarities = similarities.squeeze(0)  # Convert to 1D tensor\n        similarities = similarities.cpu().numpy()\n        indices_list = np.array(indices_list)\n\n        # Get top k indices and sort them\n        top_k = min(top_k, len(similarities))\n        top_k_indices = np.argsort(-similarities)[:top_k]\n        sorted_results = [(similarities[i], indices_list[i]) for i in top_k_indices]\n        top_k_chunks = [context_chunks[idx] for _, idx in sorted_results]\n\n        # Clear variables and GPU cache\n        del query_embedding, embeddings_list, context_embeddings, similarities\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n        return top_k_chunks\n\n    elif retriever == \"Stella\":\n        # Encode the input question\n        query_embedding = stella_model.encode(\n            [input_question], show_progress_bar=False, convert_to_tensor=True, device=\"cuda\"\n        )\n\n        # Initialize lists to store embeddings and indices\n        embeddings_list = []\n        indices_list = []\n\n        # Encode context chunks in batches\n        for i in range(0, len(context_chunks), batch_size):\n            batch_chunks = context_chunks[i:i + batch_size]\n            # Encode the batch of context chunks\n            ctx_embeddings = stella_model.encode(\n                batch_chunks, show_progress_bar=False, convert_to_tensor=True, device=\"cuda\"\n            )\n            embeddings_list.append(ctx_embeddings)\n            indices_list.extend(range(i, min(i + batch_size, len(context_chunks))))\n\n            # Clear GPU cache\n            del ctx_embeddings\n            gc.collect()\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n\n        # Concatenate all context embeddings\n        context_embeddings = torch.cat(embeddings_list, dim=0)\n\n        # Compute similarity scores\n        similarities = stella_model.similarity(query_embedding, context_embeddings)\n        similarities = similarities.squeeze(0)  # Convert to 1D tensor\n        similarities = similarities.cpu().numpy()\n        indices_list = np.array(indices_list)\n\n        # Get top k indices and sort them\n        top_k = min(top_k, len(similarities))\n        top_k_indices = np.argsort(-similarities)[:top_k]\n        sorted_results = [(similarities[i], indices_list[i]) for i in top_k_indices]\n        top_k_chunks = [context_chunks[idx] for _, idx in sorted_results]\n\n        # Clear variables and GPU cache\n        del query_embedding, embeddings_list, context_embeddings, similarities\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n        return top_k_chunks\n\n    elif retriever == \"TextAda002\":\n        # Encode the input question using text-embedding-ada-002\n        query_embedding = azure_embedding_model.embed_documents([input_question])\n\n        # Initialize lists to store embeddings and indices\n        embeddings_list = []\n        indices_list = []\n\n        # Encode context chunks in batches\n        for i in range(0, len(context_chunks), batch_size):\n            batch_chunks = context_chunks[i:i + batch_size]\n            # Encode the batch of context chunks\n            ctx_embeddings = azure_embedding_model.embed_documents(batch_chunks)\n            embeddings_list.append(ctx_embeddings)\n            indices_list.extend(range(i, min(i + batch_size, len(context_chunks))))\n\n        # Concatenate all context embeddings\n        context_embeddings = np.vstack(embeddings_list)\n\n        # Compute similarity scores using cosine similarity\n        similarities = np.dot(query_embedding, context_embeddings.T).squeeze(0)\n\n        # Get top k indices and sort them\n        top_k_indices = np.argsort(-similarities)[:top_k]\n        top_k_chunks = [context_chunks[i] for i in top_k_indices]\n\n        return top_k_chunks\n    \n    else:\n        raise ValueError(f\"Unknown retriever type: {retriever}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prompt_builder(prompt_format, context, input_question):\n    # Join the top_k_chunks into a single context string\n    context_text = \"\\n\".join(context)\n    return prompt_format.format(context=context_text, input=input_question)\n\n\nqasper_ds = load_dataset(\"THUDM/LongBench\", \"qasper\", split=\"test\")\nnarrativeqa_ds = load_dataset(\"THUDM/LongBench\", \"narrativeqa\", split=\"test\")\nmultifieldqa_en_ds = load_dataset(\"THUDM/LongBench\", \"multifieldqa_en\", split=\"test\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n\ndef invoke_azure_model(\n    ds, output_file, dataset_name, retriever=\"BM25\", batch_size=10\n):\n    if not os.path.exists(output_file):\n        with open(\"/kaggle/input/cos40011-dataset/dataset2prompt.json\", \"r\") as f:\n            dataset2prompt = json.load(f)\n        prompt_format = dataset2prompt[dataset_name]\n\n        total_samples = len(ds)\n        results = []\n\n        for index in range(total_samples):\n#             if index == 2:\n#                 break\n            row = ds[index]\n            # Retrieve context with memory management\n            context = retrieve_context(\n                row[\"context\"], row[\"input\"], retriever=retriever\n            )\n            input_question = row[\"input\"]\n            prompt = prompt_builder(prompt_format, context, input_question)\n\n            prediction = azure_model.predict(prompt)\n            result = {\n                **row,  # Unpack all key-value pairs from row\n                \"prediction\": prediction,  # Add the prediction explicitly\n            }\n            result[\"context\"] = context\n            results.append(result)\n            print(f\"Process {dataset_name} completed: {index + 1} / {total_samples} with context length: {len(context)} and {[ len(x) for x in context ]}\")\n            time.sleep(0.5)  # Pauses for 0,5 second between batch requests\n\n            # Save results periodically to prevent data loss\n            if (index + 1) % batch_size == 0 or (index + 1) == total_samples:\n                # Check if file exists to append or create new\n                if os.path.exists(output_file):\n                    existing_df = pd.read_csv(output_file)\n                    updated_df = pd.concat([existing_df, pd.DataFrame(results)], ignore_index=True)\n                else:\n                    updated_df = pd.DataFrame(results)\n                updated_df.to_csv(output_file, index=False)\n\n                # Clear results to free memory\n                results = []\n\n                # Explicitly call garbage collector and clear cache\n                gc.collect()\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n\n            # Delete variables and clear cache after each sample\n            del context, input_question, prompt, prediction, result\n            gc.collect()\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n\n    else:\n        print(f\"Output file {output_file} already exists.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_k = 5  \n\ndef rerank_context(question, contexts, use_relevance=True):\n    ranked_contexts = []\n    \n    for context in contexts:\n        if use_relevance:\n            relevance_prompt = (\n                f\"Question:\\n{question}\\n\\n\"\n                f\"Context:\\n{context}\\n\\n\"\n                \"On a scale from 1 to 10, how relevant is this context for answering the given question? \"\n                \"Please respond with a single number between 1 (least relevant) and 10 (most relevant) and do not provide any other reasoning or anything just a single number.\"\n            )\n            \n            score = azure_model.predict(relevance_prompt).strip()\n        \n        else:\n            usefulness_prompt = (\n                f\"Question:\\n{question}\\n\\n\"\n                f\"Context:\\n{context}\\n\\n\"\n                \"On a scale from 1 to 10, how useful and supportive is this context for forming a complete and accurate answer to the given question? \"\n                \"Please respond with a single number between 1 (least useful) and 10 (most useful)  and do not provide any other reasoning or anything just a single number.\"\n            )\n            \n            score = azure_model.predict(usefulness_prompt).strip()\n            \n        ranked_contexts.append((context, int(score)))\n\n    ranked_contexts = sorted(ranked_contexts, key=lambda x: x[1], reverse=True)\n    \n    top_k_contexts = [context for context, score in ranked_contexts[:top_k]]\n        \n    return top_k_contexts\n\ndef invoke_azure_model_with_reranker(\n    ds, output_file, dataset_name, retrievers=[\"BM25\", \"Dragon\"], batch_size=10, use_relevance = True\n):\n    if not os.path.exists(output_file):\n        with open(\"/kaggle/input/cos40011-dataset/dataset2prompt.json\", \"r\") as f:\n            dataset2prompt = json.load(f)\n        prompt_format = dataset2prompt[dataset_name]\n\n        total_samples = len(ds)\n        results = []\n\n        for index in range(total_samples):\n            row = ds[index]\n            \n            contexts = []\n            \n            for retriever in retrievers:\n                context = retrieve_context(\n                    row[\"context\"], row[\"input\"], retriever=retriever, top_k = 20\n                )\n                contexts.extend(context)\n            \n            input_question = row[\"input\"]\n            \n            unique_contexts = list(dict.fromkeys(contexts))\n            context = rerank_context(input_question, unique_contexts, use_relevance)\n            prompt = prompt_builder(prompt_format, context, input_question)\n\n            prediction = azure_model.predict(prompt)\n            result = {\n                **row,  # Unpack all key-value pairs from row\n                \"prediction\": prediction,  # Add the prediction explicitly\n            }\n            result[\"context\"] = context\n            results.append(result)\n            print(f\"Process {dataset_name} completed: {index + 1} / {total_samples} with context length: {len(context)} and {[ len(x) for x in context ]}\")\n            time.sleep(0.5)  # Pauses for 0,5 second between batch requests\n\n            if (index + 1) % batch_size == 0 or (index + 1) == total_samples:\n                if os.path.exists(output_file):\n                    existing_df = pd.read_csv(output_file)\n                    updated_df = pd.concat([existing_df, pd.DataFrame(results)], ignore_index=True)\n                else:\n                    updated_df = pd.DataFrame(results)\n                updated_df.to_csv(output_file, index=False)\n\n                results = []\n\n                gc.collect()\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n\n            del context, input_question, prompt, prediction, result\n            gc.collect()\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n\n    else:\n        print(f\"Output file {output_file} already exists.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"retrievers=[\"BM25\", \"Dragon\"]\nuse_relevance = True\ninvoke_azure_model_with_reranker(\n    multifieldqa_en_ds,\n    f\"multifieldqa_en_rag_reranker_{'with_relevance_prompt' if use_relevance else 'with_usefulness_prompt'}_llm_predictions.csv\",\n    \"multifieldqa_en\",\n    retrievers=retrievers,\n    use_relevance = use_relevance\n)\n\ninvoke_azure_model_with_reranker(\n    qasper_ds,\n    f\"qasper_rag_reranker_{'with_relevance_prompt' if use_relevance else 'with_usefulness_prompt'}_llm_predictions.csv\",\n    \"qasper\",\n    retrievers=retrievers,\n    use_relevance = use_relevance\n)\n\ninvoke_azure_model_with_reranker(\n    narrativeqa_ds,\n    f\"narrativeqa_rag_reranker_{'with_relevance_prompt' if use_relevance else 'with_usefulness_prompt'}_llm_predictions.csv\",\n    \"narrativeqa\",\n    retrievers=retrievers,\n    use_relevance = use_relevance\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !rm -rf /kaggle/working","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"retrievers=[\"BM25\", \"Dragon\"]\nuse_relevance = False\ninvoke_azure_model_with_reranker(\n    multifieldqa_en_ds,\n    f\"multifieldqa_en_rag_reranker_{'with_relevance_prompt' if use_relevance else 'with_usefulness_prompt'}_llm_predictions.csv\",\n    \"multifieldqa_en\",\n    retrievers=retrievers,\n    use_relevance = use_relevance\n)\n\ninvoke_azure_model_with_reranker(\n    qasper_ds,\n    f\"qasper_rag_reranker_{'with_relevance_prompt' if use_relevance else 'with_usefulness_prompt'}_llm_predictions.csv\",\n    \"qasper\",\n    retrievers=retrievers,\n    use_relevance = use_relevance\n)\n\ninvoke_azure_model_with_reranker(\n    narrativeqa_ds,\n    f\"narrativeqa_rag_reranker_{'with_relevance_prompt' if use_relevance else 'with_usefulness_prompt'}_llm_predictions.csv\",\n    \"narrativeqa\",\n    retrievers=retrievers,\n    use_relevance = use_relevance\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"retriever = \"TextAda002\"\n\n# invoke_azure_model(\n#     multifieldqa_en_ds,\n#     f\"multifieldqa_en_rag_{retriever}_llm_predictions.csv\",\n#     \"multifieldqa_en\",\n#     retriever=retriever,\n# )\n\ninvoke_azure_model(\n    qasper_ds,\n    f\"qasper_rag_{retriever}_llm_predictions.csv\",\n    \"qasper\",\n    retriever=retriever,\n)\n\ninvoke_azure_model(\n    narrativeqa_ds,\n    f\"narrativeqa_rag_{retriever}_llm_predictions.csv\",\n    \"narrativeqa\",\n    retriever=retriever,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"retriever = \"ContrieverMsmarco\"\n\ninvoke_azure_model(\n    multifieldqa_en_ds,\n    f\"multifieldqa_en_rag_{retriever}_llm_predictions.csv\",\n    \"multifieldqa_en\",\n    retriever=retriever,\n)\n\ninvoke_azure_model(\n    qasper_ds,\n    f\"qasper_rag_{retriever}_llm_predictions.csv\",\n    \"qasper\",\n    retriever=retriever,\n)\n\ninvoke_azure_model(\n    narrativeqa_ds,\n    f\"narrativeqa_rag_{retriever}_llm_predictions.csv\",\n    \"narrativeqa\",\n    retriever=retriever,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"retriever = \"Stella\"\n\ninvoke_azure_model(\n    multifieldqa_en_ds,\n    f\"multifieldqa_en_rag_{retriever}_llm_predictions.csv\",\n    \"multifieldqa_en\",\n    retriever=retriever,\n)\n\ninvoke_azure_model(\n    qasper_ds,\n    f\"qasper_rag_{retriever}_llm_predictions.csv\",\n    \"qasper\",\n    retriever=retriever,\n)\n\ninvoke_azure_model(\n    narrativeqa_ds,\n    f\"narrativeqa_rag_{retriever}_llm_predictions.csv\",\n    \"narrativeqa\",\n    retriever=retriever,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}